{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "모델의 성능을 확인하기 위한 코드이다."
      ],
      "metadata": {
        "id": "LGcY-H3SiOph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. 구글 드라이브를 코랩에 연결한다.**\n",
        "> 이는 추후 모델을 불러오고, 학습할 데이터를 불러오기 위해 필요한 과정이다. \\\n",
        " 따라서 이 코드를 실행하기 전에, 구글 드라이브에 모델과 토크나이저, 전처리된 데이터를 업로드 해야 한다."
      ],
      "metadata": {
        "id": "PUOObFrraXrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooRw3VlCY3rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43900a5f-074c-4f99-f525-97586ed82794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. 테스트를 위해 필요한 라이브러리를 불러온다.**"
      ],
      "metadata": {
        "id": "5PE-K3yt-egE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-xgz4aTYgVi",
        "outputId": "9e8deb9d-a2a5-4d90-8665-f99016306ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 22.9 MB/s \n",
            "\u001b[?25hCollecting seqeval[gpu]\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval[gpu]) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=1040895208200b201a2ee2a60899340ec5f040092e05ce6e11376e4d74386cc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tokenizers, seqeval, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 seqeval-1.2.2 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhw9YMAiYgVn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ElectraTokenizerFast, ElectraConfig, ElectraForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBTc5iX_YgVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aba1ad-58d9-4a33-af32-a422be528c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. 테스트 데이터셋을 불러온다.**\n",
        "> ***데이터의 경로 입력/수정 필수!***"
      ],
      "metadata": {
        "id": "_IBAfq80-rco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/2022_lesik_workspace/lesik/data/new_no_pool_test.tsv', sep = '\\t', keep_default_na=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9mV0kRUP5kk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "03fce984-2b75-4833-a6cf-d71055ab37ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0              2. 씻은 우람버섯은 물기를 제거하고, 포크를 이용하여 찢어 준다.   \n",
              "1  3. 포크로 찢은 우람버섯은 뜨거운 물에 소금을 넣고 30초 정도 데친 후, 찬물에...   \n",
              "2                 4. 찬물에 씻은 우람버섯은 물기를 제거하고, 그릇에 담는다.   \n",
              "3                    5. 냄비에 참기름, 다진 마늘을 넣고 센 불로 볶는다.   \n",
              "4                                1. 늙은 호박은 채 썰어 준비한다   \n",
              "\n",
              "                                               label  \n",
              "0  O O CV_STATE CV_STATE CV_INGREDIENT CV_INGREDI...  \n",
              "1  O O O O CV_STATE CV_STATE CV_INGREDIENT CV_ING...  \n",
              "2  O O O O CV_STATE CV_STATE CV_INGREDIENT CV_ING...  \n",
              "3  O O O O CV_SEASONING O CV_STATE CV_INGREDIENT ...  \n",
              "4  O O CV_INGREDIENT CV_INGREDIENT CV_INGREDIENT ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da9c0743-e5fb-47e4-9a7d-4ee5efa1998e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2. 씻은 우람버섯은 물기를 제거하고, 포크를 이용하여 찢어 준다.</td>\n",
              "      <td>O O CV_STATE CV_STATE CV_INGREDIENT CV_INGREDI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3. 포크로 찢은 우람버섯은 뜨거운 물에 소금을 넣고 30초 정도 데친 후, 찬물에...</td>\n",
              "      <td>O O O O CV_STATE CV_STATE CV_INGREDIENT CV_ING...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4. 찬물에 씻은 우람버섯은 물기를 제거하고, 그릇에 담는다.</td>\n",
              "      <td>O O O O CV_STATE CV_STATE CV_INGREDIENT CV_ING...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5. 냄비에 참기름, 다진 마늘을 넣고 센 불로 볶는다.</td>\n",
              "      <td>O O O O CV_SEASONING O CV_STATE CV_INGREDIENT ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1. 늙은 호박은 채 썰어 준비한다</td>\n",
              "      <td>O O CV_INGREDIENT CV_INGREDIENT CV_INGREDIENT ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da9c0743-e5fb-47e4-9a7d-4ee5efa1998e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da9c0743-e5fb-47e4-9a7d-4ee5efa1998e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da9c0743-e5fb-47e4-9a7d-4ee5efa1998e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "태그는 학습된 모델과 동일해야 되므로 변경해서는 안된다."
      ],
      "metadata": {
        "id": "tth3XlA1ghtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_CDSb4AZ0C5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70057662-5bb8-46dc-9073-4aa3d1448b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'TI_OTHERS', 'QT_PERCENTAGE', 'AM_REPTILIA', 'LC_OTHERS', 'QT_VOLUME', 'CV_ART', 'OGG_LIBRARY', 'FD_ART', 'OGG_LAW', 'TR_OTHERS', 'QT_AGE', 'OGG_MEDIA', 'EV_FESTIVAL', 'OGG_OTHERS', 'OGG_MILITARY', 'AF_BUILDING', 'OGG_MEDICINE', 'CV_LAW', 'TMI_EMAIL', 'LCP_COUNTY', 'TMM_DISEASE', 'CV_DRINK', 'CV_BUILDING_TYPE', 'OGG_RELIGION', 'TMI_SW', 'AF_WEAPON', 'PT_TYPE', 'CV_CULTURE', 'DT_GEOAGE', 'AM_INSECT', 'QT_SPEED', 'DT_OTHERS', 'CV_LANGUAGE', 'AM_OTHERS', 'TMI_HW', 'AFW_SERVICE_PRODUCTS', 'PS_NAME', 'CV_PRIZE', 'AM_PART', 'EV_WAR_REVOLUTION', 'TMM_DRUG', 'PT_TREE', 'TM_CLIMATE', 'QT_OTHERS', 'PS_CHARACTER', 'DT_YEAR', 'QT_LENGTH', 'OGG_SPORTS', 'PT_GRASS', 'DT_SEASON', 'CV_POLICY', 'LCG_CONTINENT', 'LCP_COUNTRY', 'AM_BIRD', 'OGG_SCIENCE', 'AF_MUSICAL_INSTRUMENT', 'CV_CLOTHING', 'CV_SEASONING', 'QT_ALBUM', 'TR_MEDICINE', 'QT_SIZE', 'OGG_EDUCATION', 'CV_FUNDS', 'MT_METAL', 'TI_DURATION', 'TMIG_GENRE', 'FD_SCIENCE', 'DT_WEEK', 'MT_ELEMENT', 'TI_HOUR', 'OGG_HOTEL', 'CV_SPORTS_INST', 'EV_OTHERS', 'LCG_RIVER', 'FD_SOCIAL_SCIENCE', 'DT_DYNASTY', 'EV_SPORTS', 'TMI_SITE', 'OGG_FOOD', 'CV_OCCUPATION', 'QT_PHONE', 'AFA_MUSIC', 'LCG_ISLAND', 'AF_CULTURAL_ASSET', 'LC_SPACE', 'CV_TRIBE', 'TR_HUMANITIES', 'PT_FLOWER', 'EV_ACTIVITY', 'TMI_MODEL', 'QT_TEMPERATURE', 'CV_TAX', 'QT_PRICE', 'FD_OTHERS', 'CV_SPORTS', 'PT_PART', 'CV_CURRENCY', 'MT_CHEMICAL', 'TM_SPORTS', 'LCG_OCEAN', 'AF_TRANSPORT', 'O', 'TM_CELL_TISSUE_ORGAN', 'DT_DURATION', 'OGG_ECONOMY', 'CV_POSITION', 'PT_FRUIT', 'AFA_ART_CRAFT', 'AM_AMPHIBIA', 'TI_SECOND', 'AM_FISH', 'PT_OTHERS', 'TR_ART', 'AFA_PERFORMANCE', 'AM_TYPE', 'FD_MEDICINE', 'CV_INGREDIENT', 'TM_SHAPE', 'CV_FOOD_STYLE', 'CV_RELATION', 'TMI_SERVICE', 'QT_SPORTS', 'TR_SCIENCE', 'TM_COLOR', 'OGG_POLITICS', 'TI_MINUTE', 'CV_SPORTS_POSITION', 'LCG_BAY', 'LCG_MOUNTAIN', 'AF_ROAD', 'DT_DAY', 'LCP_PROVINCE', 'QT_ADDRESS', 'LCP_CAPITALCITY', 'MT_ROCK', 'QT_MAN_COUNT', 'OGG_ART', 'CV_FOOD', 'TMI_PROJECT', 'LCP_CITY', 'AFA_VIDEO', 'QT_ORDER', 'AFW_OTHER_PRODUCTS', 'AM_MAMMALIA', 'FD_HUMANITIES', 'PS_PET', 'QT_CHANNEL', 'DT_MONTH', 'TM_DIRECTION', 'AFA_DOCUMENT'}\n",
            "'CV_STATE'\n",
            "{0: 'AFA_ART_CRAFT', 1: 'AFA_DOCUMENT', 2: 'AFA_MUSIC', 3: 'AFA_PERFORMANCE', 4: 'AFA_VIDEO', 5: 'AFW_OTHER_PRODUCTS', 6: 'AFW_SERVICE_PRODUCTS', 7: 'AF_BUILDING', 8: 'AF_CULTURAL_ASSET', 9: 'AF_MUSICAL_INSTRUMENT', 10: 'AF_ROAD', 11: 'AF_TRANSPORT', 12: 'AF_WEAPON', 13: 'AM_AMPHIBIA', 14: 'AM_BIRD', 15: 'AM_FISH', 16: 'AM_INSECT', 17: 'AM_MAMMALIA', 18: 'AM_OTHERS', 19: 'AM_PART', 20: 'AM_REPTILIA', 21: 'AM_TYPE', 22: 'CV_ART', 23: 'CV_BUILDING_TYPE', 24: 'CV_CLOTHING', 25: 'CV_CULTURE', 26: 'CV_CURRENCY', 27: 'CV_DRINK', 28: 'CV_FOOD', 29: 'CV_FOOD_STYLE', 30: 'CV_FUNDS', 31: 'CV_INGREDIENT', 32: 'CV_LANGUAGE', 33: 'CV_LAW', 34: 'CV_OCCUPATION', 35: 'CV_POLICY', 36: 'CV_POSITION', 37: 'CV_PRIZE', 38: 'CV_RELATION', 39: 'CV_SEASONING', 40: 'CV_SPORTS', 41: 'CV_SPORTS_INST', 42: 'CV_SPORTS_POSITION', 43: 'CV_TAX', 44: 'CV_TRIBE', 45: 'DT_DAY', 46: 'DT_DURATION', 47: 'DT_DYNASTY', 48: 'DT_GEOAGE', 49: 'DT_MONTH', 50: 'DT_OTHERS', 51: 'DT_SEASON', 52: 'DT_WEEK', 53: 'DT_YEAR', 54: 'EV_ACTIVITY', 55: 'EV_FESTIVAL', 56: 'EV_OTHERS', 57: 'EV_SPORTS', 58: 'EV_WAR_REVOLUTION', 59: 'FD_ART', 60: 'FD_HUMANITIES', 61: 'FD_MEDICINE', 62: 'FD_OTHERS', 63: 'FD_SCIENCE', 64: 'FD_SOCIAL_SCIENCE', 65: 'LCG_BAY', 66: 'LCG_CONTINENT', 67: 'LCG_ISLAND', 68: 'LCG_MOUNTAIN', 69: 'LCG_OCEAN', 70: 'LCG_RIVER', 71: 'LCP_CAPITALCITY', 72: 'LCP_CITY', 73: 'LCP_COUNTRY', 74: 'LCP_COUNTY', 75: 'LCP_PROVINCE', 76: 'LC_OTHERS', 77: 'LC_SPACE', 78: 'MT_CHEMICAL', 79: 'MT_ELEMENT', 80: 'MT_METAL', 81: 'MT_ROCK', 82: 'O', 83: 'OGG_ART', 84: 'OGG_ECONOMY', 85: 'OGG_EDUCATION', 86: 'OGG_FOOD', 87: 'OGG_HOTEL', 88: 'OGG_LAW', 89: 'OGG_LIBRARY', 90: 'OGG_MEDIA', 91: 'OGG_MEDICINE', 92: 'OGG_MILITARY', 93: 'OGG_OTHERS', 94: 'OGG_POLITICS', 95: 'OGG_RELIGION', 96: 'OGG_SCIENCE', 97: 'OGG_SPORTS', 98: 'PS_CHARACTER', 99: 'PS_NAME', 100: 'PS_PET', 101: 'PT_FLOWER', 102: 'PT_FRUIT', 103: 'PT_GRASS', 104: 'PT_OTHERS', 105: 'PT_PART', 106: 'PT_TREE', 107: 'PT_TYPE', 108: 'QT_ADDRESS', 109: 'QT_AGE', 110: 'QT_ALBUM', 111: 'QT_CHANNEL', 112: 'QT_LENGTH', 113: 'QT_MAN_COUNT', 114: 'QT_ORDER', 115: 'QT_OTHERS', 116: 'QT_PERCENTAGE', 117: 'QT_PHONE', 118: 'QT_PRICE', 119: 'QT_SIZE', 120: 'QT_SPEED', 121: 'QT_SPORTS', 122: 'QT_TEMPERATURE', 123: 'QT_VOLUME', 124: 'TI_DURATION', 125: 'TI_HOUR', 126: 'TI_MINUTE', 127: 'TI_OTHERS', 128: 'TI_SECOND', 129: 'TMIG_GENRE', 130: 'TMI_EMAIL', 131: 'TMI_HW', 132: 'TMI_MODEL', 133: 'TMI_PROJECT', 134: 'TMI_SERVICE', 135: 'TMI_SITE', 136: 'TMI_SW', 137: 'TMM_DISEASE', 138: 'TMM_DRUG', 139: 'TM_CELL_TISSUE_ORGAN', 140: 'TM_CLIMATE', 141: 'TM_COLOR', 142: 'TM_DIRECTION', 143: 'TM_SHAPE', 144: 'TM_SPORTS', 145: 'TR_ART', 146: 'TR_HUMANITIES', 147: 'TR_MEDICINE', 148: 'TR_OTHERS', 149: 'TR_SCIENCE', 150: 'CV_ACT', 151: 'CV_STATE'}\n",
            "152\n"
          ]
        }
      ],
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "arr_labels = set()\n",
        "for lb in df.label:\n",
        "    lb = lb.split()\n",
        "    for ll in lb:\n",
        "        if ll not in arr_labels:\n",
        "            arr_labels.add(ll)\n",
        "\n",
        "#말뭉치 데이터에 포함된 총 태그\n",
        "unique_labels = {'OGG_EDUCATION', 'MT_ELEMENT', 'AFW_OTHER_PRODUCTS', 'MT_ROCK', 'TI_OTHERS', 'PS_NAME', 'CV_BUILDING_TYPE', 'AM_REPTILIA', 'OGG_FOOD', 'AF_MUSICAL_INSTRUMENT', 'AF_BUILDING', 'AFA_MUSIC', 'CV_SPORTS_INST', 'QT_ORDER', 'TM_COLOR', 'LCG_MOUNTAIN', 'QT_MAN_COUNT', 'PS_CHARACTER', 'AM_OTHERS', 'OGG_LIBRARY', 'TMM_DISEASE', 'OGG_MEDICINE', 'LCG_ISLAND', 'TI_MINUTE', 'MT_CHEMICAL', 'TM_CELL_TISSUE_ORGAN', 'QT_OTHERS', 'CV_TRIBE', 'QT_TEMPERATURE', 'PT_FLOWER', 'OGG_POLITICS', 'DT_WEEK', 'FD_ART', 'AM_AMPHIBIA', 'FD_MEDICINE', 'AF_CULTURAL_ASSET', 'AF_TRANSPORT', 'EV_SPORTS', 'LCG_CONTINENT', 'PT_TREE', 'TMI_SERVICE', 'AM_MAMMALIA', 'TM_SPORTS', 'CV_INGREDIENT', 'OGG_HOTEL', 'QT_PHONE', 'CV_LANGUAGE', 'CV_FUNDS', 'CV_CURRENCY', 'FD_OTHERS', 'LCG_RIVER', 'LCP_CAPITALCITY', 'LC_OTHERS', 'QT_SIZE', 'TM_CLIMATE', 'TM_SHAPE', 'CV_POLICY', 'EV_ACTIVITY', 'TR_ART', 'QT_ADDRESS', 'OGG_RELIGION', 'CV_POSITION', 'FD_HUMANITIES', 'CV_CULTURE', 'QT_SPORTS', 'QT_ALBUM', 'CV_ART', 'CV_FOOD', 'CV_LAW', 'OGG_MILITARY', 'DT_DAY', 'FD_SOCIAL_SCIENCE', 'LCP_PROVINCE', 'CV_CLOTHING', 'TI_HOUR', 'DT_DYNASTY', 'DT_SEASON', 'FD_SCIENCE', 'TMI_HW', 'OGG_SPORTS', 'TR_OTHERS', 'TM_DIRECTION', 'TMI_SITE', 'QT_LENGTH', 'MT_METAL', 'LCG_OCEAN', 'DT_OTHERS', 'LCP_COUNTY', 'TMIG_GENRE', 'OGG_ECONOMY', 'TMI_SW', 'CV_SPORTS_POSITION', 'AFA_DOCUMENT', 'PT_OTHERS', 'AFA_ART_CRAFT', 'EV_OTHERS', 'TMI_EMAIL', 'QT_PRICE', 'EV_FESTIVAL', 'TI_SECOND', 'CV_TAX', 'O', 'QT_VOLUME', 'AF_WEAPON', 'LCG_BAY', 'OGG_SCIENCE', 'PT_FRUIT', 'CV_OCCUPATION', 'QT_CHANNEL', 'OGG_ART', 'AM_INSECT', 'CV_FOOD_STYLE', 'QT_PERCENTAGE', 'OGG_LAW', 'TR_SCIENCE', 'CV_RELATION', 'AM_PART', 'QT_AGE', 'TMI_MODEL', 'AM_BIRD', 'OGG_OTHERS', 'CV_SPORTS', 'DT_YEAR', 'LCP_COUNTRY', 'AFA_VIDEO', 'DT_GEOAGE', 'TI_DURATION', 'AM_TYPE', 'CV_SEASONING', 'AM_FISH', 'CV_PRIZE', 'PS_PET', 'AFW_SERVICE_PRODUCTS', 'TMI_PROJECT', 'CV_DRINK', 'LC_SPACE', 'LCP_CITY', 'EV_WAR_REVOLUTION', 'AFA_PERFORMANCE', 'QT_SPEED', 'PT_GRASS', 'DT_MONTH', 'PT_PART', 'OGG_MEDIA', 'PT_TYPE', 'TMM_DRUG', 'AF_ROAD', 'DT_DURATION', 'TR_MEDICINE', 'TR_HUMANITIES'}\n",
        "print(unique_labels)\n",
        "\n",
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "\n",
        "prob_tag_list=[]\n",
        "for lb in arr_labels:\n",
        "    if lb not in labels_to_ids:\n",
        "        prob_tag_list.append(\"'\"+lb+\"'\")\n",
        "print(\",\".join(prob_tag_list))\n",
        "\n",
        "#말뭉치에 포함되어 있지 않는 태그들 추가\n",
        "labels_to_ids['CV_ACT'] = 150\n",
        "ids_to_labels[150] = 'CV_ACT'\n",
        "\n",
        "labels_to_ids['CV_STATE'] = 151\n",
        "ids_to_labels[151] = 'CV_STATE'\n",
        "\n",
        "print(ids_to_labels)\n",
        "print(len(ids_to_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx5HgHShLwS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d46d55-32e4-4dab-d0ad-eb44796bf9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3. 포크로 찢은 우람버섯은 뜨거운 물에 소금을 넣고 30초 정도 데친 후, 찬물에 씻어 준다.\n",
            "172\n"
          ]
        }
      ],
      "source": [
        "# Let's take a look at how can we preprocess the text - Take first example\n",
        "text = df['text'].values.tolist()\n",
        "m_len = 0\n",
        "for t in text:\n",
        "    if m_len < len(t):\n",
        "        m_len = len(t)\n",
        "        \n",
        "example = text[1]\n",
        "\n",
        "print(example)\n",
        "print(m_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. 테스트를 진행 할 최종 모델과 토크나이저를 불러온다.**\n",
        "> ***모델, 토크나이저, epoch 입력/수정 필수!***\n",
        "\n"
      ],
      "metadata": {
        "id": "19orQlfVAOQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-dEWXr-YgVs"
      },
      "outputs": [],
      "source": [
        "def load(epoch):\n",
        "    model_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/model/FIXED_FINAL_EPOCH_'+ str(epoch) #모델 경로\n",
        "    model = ElectraForTokenClassification.from_pretrained(model_directory, num_labels=len(labels_to_ids))\n",
        "    model.to(device)\n",
        "    \n",
        "    tokenizer_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/tokenizer/FIXED_FINAL_EPOCH_' +str(epoch) #토크나이저 경로\n",
        "    tokenizer = ElectraTokenizerFast.from_pretrained(tokenizer_directory)\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "argument로 불러오기를 원하는 epoch를 적는다."
      ],
      "metadata": {
        "id": "Lu06Q-3-Arba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 72   #원하는 epoch 변경\n",
        "model, tokenizer = load(epoch)"
      ],
      "metadata": {
        "id": "7_n7KIU0AyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. 토큰화를 하기 위해 필요한 코드이다.**\n",
        "> ***원하는 epoch로 수정 가능!***"
      ],
      "metadata": {
        "id": "XnjGi75sAh0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXi-qGnGYgVu"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraTokenizerFast\n",
        "\n",
        "MAX_LEN = 256\n",
        "TEST_BATCH_SIZE = 8\n",
        "EPOCH = 72\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTaA59eJYgVv"
      },
      "outputs": [],
      "source": [
        "class ElectraDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.text[index].strip()\n",
        "        word_labels = self.data.label[index].split()\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        valid_token_list = []\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "            if mapping[0] == 0 and mapping[1] == 0:\n",
        "                continue\n",
        "            valid_token_list.append(mapping)\n",
        "        if len(valid_token_list) != len(word_labels):\n",
        "            print(index, len(word_labels), len(valid_token_list), sentence)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        if len(labels) != 0:\n",
        "            for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "                if mapping[0] == 0 and mapping[1] == 0:\n",
        "                    continue\n",
        "                tok = tokenizer.convert_ids_to_tokens(encoding['input_ids'][idx])\n",
        "            \n",
        "                # overwrite label\n",
        "                if i == len(labels):\n",
        "                    break\n",
        "                encoded_labels[idx] = labels[i]\n",
        "                i += 1\n",
        "                \n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['label'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set = ElectraDataset(df, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "C4NKG0d9BiDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cv53qH97a8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fd5d82-33b6-4677-bae9-be2ebfe75627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODYsOGm_YgVx"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knuKPJOiHbiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66cd686-34b4-4409-8aaf-be0cb134fb38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "inputs = testing_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"label\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URciATH5YgVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b2a3ac-1e21-4597-ec14-ed92c15bd0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 152])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSjVszsAYgVy"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. test 함수를 불러오는 섹션이다.**"
      ],
      "metadata": {
        "id": "BsQkPbB5B5EF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjRtfFFJYgVy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def test():\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    nb_test_examples, nb_test_steps = 0, 0\n",
        "    test_preds, test_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['label'].to(device, dtype = torch.long)\n",
        "            \n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss = output[0]\n",
        "            test_logits = output[1]\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            nb_test_steps += 1\n",
        "            nb_test_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = test_loss/nb_test_steps\n",
        "                print(f\"Test loss per 100 test steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = test_logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            test_labels.extend(labels)\n",
        "            test_preds.extend(predictions)\n",
        "            \n",
        "            tmp_test_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            test_accuracy += tmp_test_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in test_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in test_preds]\n",
        "    print(classification_report(labels, predictions))\n",
        "    test_loss = test_loss / nb_test_steps\n",
        "    test_accuracy = test_accuracy / nb_test_steps\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEST 결과**"
      ],
      "metadata": {
        "id": "8LzfWnTkCCGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJbOtD9fYnN1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**(선택) predict 함수이다.**"
      ],
      "metadata": {
        "id": "D7YIn6zsbFtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#예측하고 싶은 텍스트를 넣어주세요.\n",
        "predict_text = '다진 마늘을 넣고 잘게 자른 김치를 섞어주세요.'"
      ],
      "metadata": {
        "id": "ajhENtTODgLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    result = []\n",
        "    inputs = tokenizer(sentence,\n",
        "                        return_offsets_mapping=True, \n",
        "                        padding='max_length', \n",
        "                        truncation=True, \n",
        "                        max_length=MAX_LEN,\n",
        "                        return_tensors=\"pt\")\n",
        "\n",
        "    # move to gpu\n",
        "    ids = inputs[\"input_ids\"].to(device)\n",
        "    mask = inputs[\"attention_mask\"].to(device)\n",
        "    # forward pass\n",
        "    outputs = model(ids, attention_mask=mask)\n",
        "    logits = outputs[0]\n",
        "\n",
        "    active_logits = logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
        "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "    word_prediction = []\n",
        "    token_prediction = []\n",
        "    pred_dict = {}\n",
        "    token_dict = {}\n",
        "    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n",
        "        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n",
        "            result.append([word_idx, token_pred[0], token_pred[1]]) # \n",
        "            #print(token_pred)\n",
        "            if word_idx not in pred_dict:\n",
        "                pred_dict[word_idx] = set()\n",
        "            if word_idx not in token_dict:\n",
        "                token_dict[word_idx] = \"\"\n",
        "            pred_dict[word_idx].add(token_pred[1])\n",
        "            token_dict[word_idx] += token_pred[0].replace(\"#\", \"\")\n",
        "                \n",
        "    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n",
        "        #only predictions on first word pieces are important\n",
        "        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n",
        "            if token_pred[1] != 'O':\n",
        "                token_prediction.append([word_idx, token_pred[0],token_pred[1]])\n",
        "    \n",
        "    #for i in range(len(token_prediction)):\n",
        "    #    print(token_prediction[i], word_idx)\n",
        "    #print()\n",
        "    return result"
      ],
      "metadata": {
        "id": "e7iahMTihUWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JfupT4vYgV0",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66413dea-0d62-4e07-8526-6cb0906829b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, '다진', 'CV_STATE'],\n",
              " [1, '마늘', 'CV_INGREDIENT'],\n",
              " [1, '##을', 'O'],\n",
              " [2, '넣', 'O'],\n",
              " [2, '##고', 'O'],\n",
              " [3, '잘', 'CV_STATE'],\n",
              " [3, '##게', 'CV_STATE'],\n",
              " [4, '자', 'CV_STATE'],\n",
              " [4, '##른', 'CV_STATE'],\n",
              " [5, '김치', 'CV_INGREDIENT'],\n",
              " [5, '##를', 'O'],\n",
              " [6, '섞', 'O'],\n",
              " [6, '##어', 'O'],\n",
              " [6, '##주', 'O'],\n",
              " [6, '##세요', 'O'],\n",
              " [7, '.', 'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "predict(predict_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "PyTorch 1.13 (NGC 22.05/Python 3.8 Conda) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}