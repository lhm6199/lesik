{"cells":[{"cell_type":"markdown","metadata":{"id":"LGcY-H3SiOph"},"source":["모델의 성능을 확인하기 위한 코드이다."]},{"cell_type":"markdown","metadata":{"id":"PUOObFrraXrY"},"source":["#**1. 구글 드라이브를 코랩에 연결한다.**\n","> 이는 추후 모델을 불러오고, 학습할 데이터를 불러오기 위해 필요한 과정이다. \\\n"," 따라서 이 코드를 실행하기 전에, 구글 드라이브에 모델과 토크나이저, 전처리된 데이터를 업로드 해야 한다."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20265,"status":"ok","timestamp":1671011072425,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"ooRw3VlCY3rE","outputId":"7c5f73ec-4197-4276-c930-67973687ac7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"5PE-K3yt-egE"},"source":["#**2. 테스트를 위해 필요한 라이브러리를 불러온다.**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14758,"status":"ok","timestamp":1671011087160,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"l-xgz4aTYgVi","outputId":"f1c8d79d-7f3f-423b-e03e-4733a9337ef7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 5.1 MB/s \n","\u001b[?25hCollecting seqeval[gpu]\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 73 kB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 63.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval[gpu]) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=dc60519f506f90da25fd9a2896b1565aed94bf37c4eab86ed93ba046e51a92c5\n","  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n","Successfully built seqeval\n","Installing collected packages: tokenizers, seqeval, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 seqeval-1.2.2 tokenizers-0.13.2 transformers-4.25.1\n"]}],"source":["!pip install transformers seqeval[gpu]"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7825,"status":"ok","timestamp":1671011094974,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"Qhw9YMAiYgVn"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import ElectraTokenizerFast, ElectraConfig, ElectraForTokenClassification"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":815,"status":"ok","timestamp":1671011095759,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"XBTc5iX_YgVo","outputId":"aa517412-53fc-4114-89b1-a4118d6db3f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"_IBAfq80-rco"},"source":["#**3. 테스트 데이터셋을 불러온다.**\n","> ***데이터의 경로 입력/수정 필수!***"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1671011164337,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"9mV0kRUP5kk9","outputId":"7f8c9c5e-e1c6-4390-e678-42f700da4faf"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-539040ec-4d53-4790-a3d6-3ad12c1ce430\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1. 어묵과 양파, 청피망은 얇게 채 썰고 대파는 3등분해 길게 채 썰어주세요.</td>\n","      <td>O O CV_INGREDIENT O CV_INGREDIENT O CV_INGREDI...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2. 오징어는 깨끗이 씻어 얇게 썬 다음 끓는 물에 살짝 데쳐내주세요.</td>\n","      <td>O O CV_INGREDIENT O O O O O O O O O O O O O O ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4. 어묵과 오징어, 분량의 양념을 넣고 빠르게 볶아낸 후 대파와 올리고당, 참기름...</td>\n","      <td>O O CV_INGREDIENT O CV_INGREDIENT O O O O O O ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5. 갓 지어낸 솥밥을 접시에 퍼담고 오징어 어묵 볶음을 옆에 예쁘게 담아 완성해주세요.</td>\n","      <td>O O O O O O CV_INGREDIENT O O O O O O O O O O ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2. 만가닥버섯은 밑동 부분을 잘라 먹기 좋게 나누고 양파는 채 썰어주세요.</td>\n","      <td>O O CV_INGREDIENT CV_INGREDIENT CV_INGREDIENT ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-539040ec-4d53-4790-a3d6-3ad12c1ce430')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-539040ec-4d53-4790-a3d6-3ad12c1ce430 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-539040ec-4d53-4790-a3d6-3ad12c1ce430');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                text  \\\n","0       1. 어묵과 양파, 청피망은 얇게 채 썰고 대파는 3등분해 길게 채 썰어주세요.   \n","1            2. 오징어는 깨끗이 씻어 얇게 썬 다음 끓는 물에 살짝 데쳐내주세요.   \n","2  4. 어묵과 오징어, 분량의 양념을 넣고 빠르게 볶아낸 후 대파와 올리고당, 참기름...   \n","3  5. 갓 지어낸 솥밥을 접시에 퍼담고 오징어 어묵 볶음을 옆에 예쁘게 담아 완성해주세요.   \n","4         2. 만가닥버섯은 밑동 부분을 잘라 먹기 좋게 나누고 양파는 채 썰어주세요.   \n","\n","                                               label  \n","0  O O CV_INGREDIENT O CV_INGREDIENT O CV_INGREDI...  \n","1  O O CV_INGREDIENT O O O O O O O O O O O O O O ...  \n","2  O O CV_INGREDIENT O CV_INGREDIENT O O O O O O ...  \n","3  O O O O O O CV_INGREDIENT O O O O O O O O O O ...  \n","4  O O CV_INGREDIENT CV_INGREDIENT CV_INGREDIENT ...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","df = pd.read_csv('/content/gdrive/MyDrive/2022_lesik_workspace/lesik/data/total+yejib_test.tsv', sep = '\\t', keep_default_na=False)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"tth3XlA1ghtU"},"source":["태그는 학습된 모델과 동일해야 되므로 변경해서는 안된다."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1671011164852,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"S_CDSb4AZ0C5","outputId":"59d0d587-a851-41b3-bec8-b550c508bc24"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'CV_INGREDIENT', 'CV_SEASONING', 'CV_STATE', 'TI_DURATION', 'QT_VOLUME', 'QT_TEMPERATURE', 'O'}\n","{'TM_DIRECTION', 'OGG_POLITICS', 'TMIG_GENRE', 'DT_GEOAGE', 'LCP_PROVINCE', 'TMI_MODEL', 'AFA_VIDEO', 'CV_CURRENCY', 'OGG_HOTEL', 'EV_OTHERS', 'QT_PERCENTAGE', 'AF_BUILDING', 'FD_MEDICINE', 'CV_FOOD', 'CV_SEASONING', 'OGG_LIBRARY', 'CV_RELATION', 'TR_ART', 'CV_OCCUPATION', 'FD_ART', 'AFW_SERVICE_PRODUCTS', 'CV_SPORTS_INST', 'DT_YEAR', 'DT_DAY', 'TM_COLOR', 'AM_MAMMALIA', 'QT_PHONE', 'CV_CLOTHING', 'TMM_DRUG', 'DT_DYNASTY', 'CV_POSITION', 'TM_SHAPE', 'AM_PART', 'QT_AGE', 'FD_OTHERS', 'TMI_HW', 'MT_ELEMENT', 'OGG_RELIGION', 'OGG_LAW', 'CV_SPORTS', 'LCG_MOUNTAIN', 'TI_SECOND', 'AM_INSECT', 'EV_FESTIVAL', 'QT_SPEED', 'FD_HUMANITIES', 'LCP_COUNTY', 'EV_WAR_REVOLUTION', 'CV_CULTURE', 'CV_PRIZE', 'LC_SPACE', 'AFA_DOCUMENT', 'TI_DURATION', 'TMI_SERVICE', 'QT_MAN_COUNT', 'CV_LANGUAGE', 'QT_TEMPERATURE', 'AF_WEAPON', 'TR_HUMANITIES', 'CV_FUNDS', 'AF_ROAD', 'OGG_SCIENCE', 'CV_INGREDIENT', 'OGG_ECONOMY', 'QT_ORDER', 'LCP_CITY', 'QT_OTHERS', 'CV_TRIBE', 'LC_OTHERS', 'OGG_MILITARY', 'PT_FRUIT', 'TM_CLIMATE', 'AM_BIRD', 'AFA_ART_CRAFT', 'AM_REPTILIA', 'O', 'CV_LAW', 'LCG_CONTINENT', 'CV_BUILDING_TYPE', 'AM_AMPHIBIA', 'AM_TYPE', 'PS_NAME', 'AM_FISH', 'TMI_SITE', 'CV_FOOD_STYLE', 'LCG_RIVER', 'TI_HOUR', 'MT_METAL', 'DT_WEEK', 'AF_TRANSPORT', 'QT_LENGTH', 'OGG_MEDICINE', 'DT_MONTH', 'QT_VOLUME', 'LCG_ISLAND', 'AF_MUSICAL_INSTRUMENT', 'PT_TREE', 'DT_SEASON', 'CV_DRINK', 'TR_SCIENCE', 'PS_PET', 'TMI_PROJECT', 'LCG_BAY', 'TMM_DISEASE', 'TI_OTHERS', 'LCP_CAPITALCITY', 'EV_ACTIVITY', 'CV_TAX', 'LCP_COUNTRY', 'PT_PART', 'TMI_EMAIL', 'OGG_FOOD', 'OGG_OTHERS', 'CV_POLICY', 'DT_OTHERS', 'TI_MINUTE', 'CV_ART', 'FD_SCIENCE', 'QT_PRICE', 'TM_SPORTS', 'AF_CULTURAL_ASSET', 'OGG_ART', 'QT_ALBUM', 'CV_SPORTS_POSITION', 'PT_GRASS', 'OGG_EDUCATION', 'TM_CELL_TISSUE_ORGAN', 'TR_MEDICINE', 'PT_FLOWER', 'MT_ROCK', 'TR_OTHERS', 'LCG_OCEAN', 'OGG_MEDIA', 'QT_SIZE', 'AFA_PERFORMANCE', 'AFA_MUSIC', 'MT_CHEMICAL', 'QT_ADDRESS', 'PT_TYPE', 'OGG_SPORTS', 'AM_OTHERS', 'QT_CHANNEL', 'DT_DURATION', 'PS_CHARACTER', 'AFW_OTHER_PRODUCTS', 'QT_SPORTS', 'TMI_SW', 'EV_SPORTS', 'FD_SOCIAL_SCIENCE', 'PT_OTHERS'}\n","'CV_STATE'\n","{0: 'AFA_ART_CRAFT', 1: 'AFA_DOCUMENT', 2: 'AFA_MUSIC', 3: 'AFA_PERFORMANCE', 4: 'AFA_VIDEO', 5: 'AFW_OTHER_PRODUCTS', 6: 'AFW_SERVICE_PRODUCTS', 7: 'AF_BUILDING', 8: 'AF_CULTURAL_ASSET', 9: 'AF_MUSICAL_INSTRUMENT', 10: 'AF_ROAD', 11: 'AF_TRANSPORT', 12: 'AF_WEAPON', 13: 'AM_AMPHIBIA', 14: 'AM_BIRD', 15: 'AM_FISH', 16: 'AM_INSECT', 17: 'AM_MAMMALIA', 18: 'AM_OTHERS', 19: 'AM_PART', 20: 'AM_REPTILIA', 21: 'AM_TYPE', 22: 'CV_ART', 23: 'CV_BUILDING_TYPE', 24: 'CV_CLOTHING', 25: 'CV_CULTURE', 26: 'CV_CURRENCY', 27: 'CV_DRINK', 28: 'CV_FOOD', 29: 'CV_FOOD_STYLE', 30: 'CV_FUNDS', 31: 'CV_INGREDIENT', 32: 'CV_LANGUAGE', 33: 'CV_LAW', 34: 'CV_OCCUPATION', 35: 'CV_POLICY', 36: 'CV_POSITION', 37: 'CV_PRIZE', 38: 'CV_RELATION', 39: 'CV_SEASONING', 40: 'CV_SPORTS', 41: 'CV_SPORTS_INST', 42: 'CV_SPORTS_POSITION', 43: 'CV_TAX', 44: 'CV_TRIBE', 45: 'DT_DAY', 46: 'DT_DURATION', 47: 'DT_DYNASTY', 48: 'DT_GEOAGE', 49: 'DT_MONTH', 50: 'DT_OTHERS', 51: 'DT_SEASON', 52: 'DT_WEEK', 53: 'DT_YEAR', 54: 'EV_ACTIVITY', 55: 'EV_FESTIVAL', 56: 'EV_OTHERS', 57: 'EV_SPORTS', 58: 'EV_WAR_REVOLUTION', 59: 'FD_ART', 60: 'FD_HUMANITIES', 61: 'FD_MEDICINE', 62: 'FD_OTHERS', 63: 'FD_SCIENCE', 64: 'FD_SOCIAL_SCIENCE', 65: 'LCG_BAY', 66: 'LCG_CONTINENT', 67: 'LCG_ISLAND', 68: 'LCG_MOUNTAIN', 69: 'LCG_OCEAN', 70: 'LCG_RIVER', 71: 'LCP_CAPITALCITY', 72: 'LCP_CITY', 73: 'LCP_COUNTRY', 74: 'LCP_COUNTY', 75: 'LCP_PROVINCE', 76: 'LC_OTHERS', 77: 'LC_SPACE', 78: 'MT_CHEMICAL', 79: 'MT_ELEMENT', 80: 'MT_METAL', 81: 'MT_ROCK', 82: 'O', 83: 'OGG_ART', 84: 'OGG_ECONOMY', 85: 'OGG_EDUCATION', 86: 'OGG_FOOD', 87: 'OGG_HOTEL', 88: 'OGG_LAW', 89: 'OGG_LIBRARY', 90: 'OGG_MEDIA', 91: 'OGG_MEDICINE', 92: 'OGG_MILITARY', 93: 'OGG_OTHERS', 94: 'OGG_POLITICS', 95: 'OGG_RELIGION', 96: 'OGG_SCIENCE', 97: 'OGG_SPORTS', 98: 'PS_CHARACTER', 99: 'PS_NAME', 100: 'PS_PET', 101: 'PT_FLOWER', 102: 'PT_FRUIT', 103: 'PT_GRASS', 104: 'PT_OTHERS', 105: 'PT_PART', 106: 'PT_TREE', 107: 'PT_TYPE', 108: 'QT_ADDRESS', 109: 'QT_AGE', 110: 'QT_ALBUM', 111: 'QT_CHANNEL', 112: 'QT_LENGTH', 113: 'QT_MAN_COUNT', 114: 'QT_ORDER', 115: 'QT_OTHERS', 116: 'QT_PERCENTAGE', 117: 'QT_PHONE', 118: 'QT_PRICE', 119: 'QT_SIZE', 120: 'QT_SPEED', 121: 'QT_SPORTS', 122: 'QT_TEMPERATURE', 123: 'QT_VOLUME', 124: 'TI_DURATION', 125: 'TI_HOUR', 126: 'TI_MINUTE', 127: 'TI_OTHERS', 128: 'TI_SECOND', 129: 'TMIG_GENRE', 130: 'TMI_EMAIL', 131: 'TMI_HW', 132: 'TMI_MODEL', 133: 'TMI_PROJECT', 134: 'TMI_SERVICE', 135: 'TMI_SITE', 136: 'TMI_SW', 137: 'TMM_DISEASE', 138: 'TMM_DRUG', 139: 'TM_CELL_TISSUE_ORGAN', 140: 'TM_CLIMATE', 141: 'TM_COLOR', 142: 'TM_DIRECTION', 143: 'TM_SHAPE', 144: 'TM_SPORTS', 145: 'TR_ART', 146: 'TR_HUMANITIES', 147: 'TR_MEDICINE', 148: 'TR_OTHERS', 149: 'TR_SCIENCE', 150: 'CV_ACT', 151: 'CV_STATE'}\n","152\n"]}],"source":["# Split labels based on whitespace and turn them into a list\n","arr_labels = set()\n","for lb in df.label:\n","    lb = lb.split()\n","    for ll in lb:\n","        if ll not in arr_labels:\n","            arr_labels.add(ll)\n","\n","#말뭉치 데이터에 포함된 총 태그\n","unique_labels = {'OGG_EDUCATION', 'MT_ELEMENT', 'AFW_OTHER_PRODUCTS', 'MT_ROCK', 'TI_OTHERS', 'PS_NAME', 'CV_BUILDING_TYPE', 'AM_REPTILIA', 'OGG_FOOD', 'AF_MUSICAL_INSTRUMENT', 'AF_BUILDING', 'AFA_MUSIC', 'CV_SPORTS_INST', 'QT_ORDER', 'TM_COLOR', 'LCG_MOUNTAIN', 'QT_MAN_COUNT', 'PS_CHARACTER', 'AM_OTHERS', 'OGG_LIBRARY', 'TMM_DISEASE', 'OGG_MEDICINE', 'LCG_ISLAND', 'TI_MINUTE', 'MT_CHEMICAL', 'TM_CELL_TISSUE_ORGAN', 'QT_OTHERS', 'CV_TRIBE', 'QT_TEMPERATURE', 'PT_FLOWER', 'OGG_POLITICS', 'DT_WEEK', 'FD_ART', 'AM_AMPHIBIA', 'FD_MEDICINE', 'AF_CULTURAL_ASSET', 'AF_TRANSPORT', 'EV_SPORTS', 'LCG_CONTINENT', 'PT_TREE', 'TMI_SERVICE', 'AM_MAMMALIA', 'TM_SPORTS', 'CV_INGREDIENT', 'OGG_HOTEL', 'QT_PHONE', 'CV_LANGUAGE', 'CV_FUNDS', 'CV_CURRENCY', 'FD_OTHERS', 'LCG_RIVER', 'LCP_CAPITALCITY', 'LC_OTHERS', 'QT_SIZE', 'TM_CLIMATE', 'TM_SHAPE', 'CV_POLICY', 'EV_ACTIVITY', 'TR_ART', 'QT_ADDRESS', 'OGG_RELIGION', 'CV_POSITION', 'FD_HUMANITIES', 'CV_CULTURE', 'QT_SPORTS', 'QT_ALBUM', 'CV_ART', 'CV_FOOD', 'CV_LAW', 'OGG_MILITARY', 'DT_DAY', 'FD_SOCIAL_SCIENCE', 'LCP_PROVINCE', 'CV_CLOTHING', 'TI_HOUR', 'DT_DYNASTY', 'DT_SEASON', 'FD_SCIENCE', 'TMI_HW', 'OGG_SPORTS', 'TR_OTHERS', 'TM_DIRECTION', 'TMI_SITE', 'QT_LENGTH', 'MT_METAL', 'LCG_OCEAN', 'DT_OTHERS', 'LCP_COUNTY', 'TMIG_GENRE', 'OGG_ECONOMY', 'TMI_SW', 'CV_SPORTS_POSITION', 'AFA_DOCUMENT', 'PT_OTHERS', 'AFA_ART_CRAFT', 'EV_OTHERS', 'TMI_EMAIL', 'QT_PRICE', 'EV_FESTIVAL', 'TI_SECOND', 'CV_TAX', 'O', 'QT_VOLUME', 'AF_WEAPON', 'LCG_BAY', 'OGG_SCIENCE', 'PT_FRUIT', 'CV_OCCUPATION', 'QT_CHANNEL', 'OGG_ART', 'AM_INSECT', 'CV_FOOD_STYLE', 'QT_PERCENTAGE', 'OGG_LAW', 'TR_SCIENCE', 'CV_RELATION', 'AM_PART', 'QT_AGE', 'TMI_MODEL', 'AM_BIRD', 'OGG_OTHERS', 'CV_SPORTS', 'DT_YEAR', 'LCP_COUNTRY', 'AFA_VIDEO', 'DT_GEOAGE', 'TI_DURATION', 'AM_TYPE', 'CV_SEASONING', 'AM_FISH', 'CV_PRIZE', 'PS_PET', 'AFW_SERVICE_PRODUCTS', 'TMI_PROJECT', 'CV_DRINK', 'LC_SPACE', 'LCP_CITY', 'EV_WAR_REVOLUTION', 'AFA_PERFORMANCE', 'QT_SPEED', 'PT_GRASS', 'DT_MONTH', 'PT_PART', 'OGG_MEDIA', 'PT_TYPE', 'TMM_DRUG', 'AF_ROAD', 'DT_DURATION', 'TR_MEDICINE', 'TR_HUMANITIES'}\n","print(unique_labels)\n","\n","# Map each label into its id representation and vice versa\n","labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n","ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n","\n","prob_tag_list=[]\n","for lb in arr_labels:\n","    if lb not in labels_to_ids:\n","        prob_tag_list.append(\"'\"+lb+\"'\")\n","print(\",\".join(prob_tag_list))\n","\n","#말뭉치에 포함되어 있지 않는 태그들 추가\n","labels_to_ids['CV_ACT'] = 150\n","ids_to_labels[150] = 'CV_ACT'\n","\n","labels_to_ids['CV_STATE'] = 151\n","ids_to_labels[151] = 'CV_STATE'\n","\n","print(ids_to_labels)\n","print(len(ids_to_labels))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1671011164855,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"lx5HgHShLwS0","outputId":"bae7283a-e3e4-4fb3-b131-107c4d1aa9e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["2. 오징어는 깨끗이 씻어 얇게 썬 다음 끓는 물에 살짝 데쳐내주세요.\n","172\n"]}],"source":["# Let's take a look at how can we preprocess the text - Take first example\n","text = df['text'].values.tolist()\n","m_len = 0\n","for t in text:\n","    if m_len < len(t):\n","        m_len = len(t)\n","        \n","example = text[1]\n","\n","print(example)\n","print(m_len)"]},{"cell_type":"markdown","metadata":{"id":"19orQlfVAOQP"},"source":["#**4. 테스트를 진행 할 최종 모델과 토크나이저를 불러온다.**\n","> ***모델, 토크나이저, epoch 입력/수정 필수!***\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1671011172037,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"B-dEWXr-YgVs"},"outputs":[],"source":["def load(epoch):\n","    model_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/model/FIXED_FINAL_EPOCH_'+ str(epoch)\n","    model = ElectraForTokenClassification.from_pretrained(model_directory, num_labels=len(labels_to_ids))\n","    model.to(device)\n","    \n","    tokenizer_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/tokenizer/FIXED_FINAL_EPOCH_' +str(epoch)\n","    tokenizer = ElectraTokenizerFast.from_pretrained(tokenizer_directory)\n","    \n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"Lu06Q-3-Arba"},"source":["argument로 불러오기를 원하는 epoch를 적는다."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9807,"status":"ok","timestamp":1671011182123,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"7_n7KIU0AyMd"},"outputs":[],"source":["epoch = 72\n","model, tokenizer = load(epoch)"]},{"cell_type":"markdown","metadata":{"id":"XnjGi75sAh0e"},"source":["#**5. 토큰화를 하기 위해 필요한 코드이다.**\n","> ***원하는 epoch로 수정 가능!***"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1671011217979,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"iXi-qGnGYgVu"},"outputs":[],"source":["from transformers import ElectraTokenizerFast\n","\n","MAX_LEN = 256\n","TEST_BATCH_SIZE = 8\n","EPOCH = 72\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1671011218285,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"OTaA59eJYgVv"},"outputs":[],"source":["class ElectraDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, index):\n","        # step 1: get the sentence and word labels \n","        sentence = self.data.text[index].strip()\n","        word_labels = self.data.label[index].split()\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                             return_offsets_mapping=True, \n","                             padding='max_length', \n","                             truncation=True, \n","                             max_length=self.max_len)\n","        \n","        valid_token_list = []\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","            if mapping[0] == 0 and mapping[1] == 0:\n","                continue\n","            valid_token_list.append(mapping)\n","        if len(valid_token_list) != len(word_labels):\n","            print(index, len(word_labels), len(valid_token_list), sentence)\n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","        labels = [labels_to_ids[label] for label in word_labels] \n","        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        if len(labels) != 0:\n","            for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","                if mapping[0] == 0 and mapping[1] == 0:\n","                    continue\n","                tok = tokenizer.convert_ids_to_tokens(encoding['input_ids'][idx])\n","            \n","                # overwrite label\n","                if i == len(labels):\n","                    break\n","                encoded_labels[idx] = labels[i]\n","                i += 1\n","                \n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['label'] = torch.as_tensor(encoded_labels)\n","        \n","        return item\n","    def __len__(self):\n","        return self.len"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1671011218286,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"C4NKG0d9BiDP"},"outputs":[],"source":["testing_set = ElectraDataset(df, tokenizer, MAX_LEN)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1671011218287,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"8Cv53qH97a8P","outputId":"b008e54e-a9a4-4445-d8a3-a32ea84b99d3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["test_params = {'batch_size': TEST_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 4\n","                }\n","\n","testing_loader = DataLoader(testing_set, **test_params)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1671011218288,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"ODYsOGm_YgVx","outputId":"09f37074-3e72-455e-8e77-e5b604105ef8"},"outputs":[{"data":{"text/plain":["ElectraForTokenClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(32200, 128, padding_idx=0)\n","      (position_embeddings): Embedding(512, 128)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=256, out_features=256, bias=True)\n","              (key): Linear(in_features=256, out_features=256, bias=True)\n","              (value): Linear(in_features=256, out_features=256, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=256, out_features=256, bias=True)\n","              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=256, out_features=1024, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=1024, out_features=256, bias=True)\n","            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=256, out_features=152, bias=True)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3345,"status":"ok","timestamp":1671011221615,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"knuKPJOiHbiZ","outputId":"0b967868-0301-42eb-87b5-d36c443606c0"},"outputs":[{"data":{"text/plain":["tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["inputs = testing_set[2]\n","input_ids = inputs[\"input_ids\"].unsqueeze(0)\n","attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n","labels = inputs[\"label\"].unsqueeze(0)\n","\n","input_ids = input_ids.to(device)\n","attention_mask = attention_mask.to(device)\n","labels = labels.to(device)\n","\n","outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","initial_loss = outputs[0]\n","initial_loss"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1671011221617,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"URciATH5YgVy","outputId":"e3a6e8b5-2b2a-4d5c-d930-cebf1ff3dcd5"},"outputs":[{"data":{"text/plain":["torch.Size([1, 256, 152])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tr_logits = outputs[1]\n","tr_logits.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1671011221620,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"iSjVszsAYgVy"},"outputs":[],"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"BsQkPbB5B5EF"},"source":["#**6. test 함수를 불러오는 섹션이다.**"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1671011435210,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"pjRtfFFJYgVy"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","def test():\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    test_loss, test_accuracy = 0, 0\n","    nb_test_examples, nb_test_steps = 0, 0\n","    test_preds, test_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['label'].to(device, dtype = torch.long)\n","            \n","            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = output[0]\n","            test_logits = output[1]\n","            \n","            test_loss += loss.item()\n","\n","            nb_test_steps += 1\n","            nb_test_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = test_loss/nb_test_steps\n","                print(f\"Test loss per 100 test steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = test_logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            test_labels.extend(labels)\n","            test_preds.extend(predictions)\n","            \n","            tmp_test_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            test_accuracy += tmp_test_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in test_labels]\n","    predictions = [ids_to_labels[id.item()] for id in test_preds]\n","    print(classification_report(labels, predictions))\n","    test_loss = test_loss / nb_test_steps\n","    test_accuracy = test_accuracy / nb_test_steps\n","    print(f\"Test Loss: {test_loss}\")\n","    print(f\"Test Accuracy: {test_accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"8LzfWnTkCCGN"},"source":["#**TEST 결과**"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6389,"status":"ok","timestamp":1671011443350,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"cJbOtD9fYnN1","outputId":"b4e1b81e-4d3e-44d7-c14e-098ef985bb30","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"name":"stdout","output_type":"stream","text":["1001 14 13 12. 호박잎은 바닥에 하나씩 펼쳐주세요.\n","Test loss per 100 test steps: 0.10267500579357147\n","1145 32 31 10. 달군 프라이팬에 식용유를 두르고 반죽을 한 수저씩 떠 넣으면서 앞뒤로 노릇노릇하게 부쳐줍니다.\n","840 17 16 14. 식은 약밥은 원하는 크기와 모양으로 자른다.\n","1112 30 29 11. 콩나물,청경채,버섯,청양고추,후추를 넣어 콩나물이 익을때까지 끓여주어 완성.\n","823 16 15 10. 냄비에 삶은 고기, 양념을 넣고 버무린다.\n","1003 99230 23  2229 10. 다진마늘을 넣어서 함께 양념장을 잘 섞이도록 저어주세요.\n"," 16. 묵은지-고기쌈장-양념해놓은밥-돌돌 야무지게 여미듯 말아주세요.\n","953 18 17 14. 튀긴 고로케는 체반에 올려 기름을 충분히 제거한다.\n","544 26 25 10. 완성된 단호박 해물 떡 찜을 접시에 담고 먹기 좋게 잘라 맛있게 즐겨주세요.\n","746 40 39 10. 약한 불에서 팬에 기름을 두른 후 닦아내고 숟가락으로 밀전병 반죽을 한술 떠 팬에 붓고 둥글고 얇게 펼쳐 부쳐주세요.\n","993 15 14 12. 양념장에 구운두부와 함께 넣어주세요.\n","815 12 11 10. 끓인 라면은 그릇에 담는다.\n","496 23 22 11. 접시에 준비한 재료를 둥글게 돌려 담고 가운데 전병을 올려 완성해주세요.\n","944 26 25 10. 끓어오르면 약불로 줄이고 설탕에 버무린 살를 넣고 5분 정도 끓인다.\n","Test loss per 100 test steps: 0.10969887605158261\n","1089 25 24 10. 밥에 참기름1T, 맛소금0.8티스푼을 넣어 잘 섞어줍니다.\n","856 24 23 10. 익힌 광어는 그릇에 담아 식혀 준 뒤, 마요네즈를 넣고 버무린다.\n","854 17 16 11. 자른 김 가운데 타원형으로 만든 밥을 넣어 준다.\n","866 18 17 14. 속을 채운 오징어 끝에 이쑤시개를 꼽아 준다.\n","994 14 13 13. 구운두부와 양념장을 넣어주세요.\n","853 17 16 10. 참기름에 버무린 밥은 손을 이용하여 타원형으로 만든다.\n","1090 26 25 11. 김에 양념된 밥을 얇게 잘 펴준뒤, 달걀말이를 속에 넣어줍니다.\n","951 15 14 11. 크림치즈를 넣은 감자는 동그랗게 만든다.\n","66332  31 10. 완성된 식혜를 냄비에 붓고 갈아둔 호박과 설탕을 넣어 저어가며 5분간 끓여 완성해주세요.\n","952 21 20 12. 동그랗게 만든 감자는 밀가루, 계란물, 빵가루 순서대로 묻혀준다.\n","1002 26  2514. 호박잎-고기쌈장-양념해놓은밥-돌돌 야무지게 말아주세요.\n","865  1918 10. 물기를 제거한 당면은 가위나 칼을 이용하여 잘게 자른다.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["                precision    recall  f1-score   support\n","\n"," CV_INGREDIENT       0.91      0.96      0.93      3377\n","  CV_SEASONING       0.89      0.93      0.91      1805\n","      CV_STATE       0.87      0.94      0.90      1352\n","  LCP_PROVINCE       0.00      0.00      0.00         0\n","             O       0.99      0.98      0.98     28804\n"," QT_PERCENTAGE       0.00      0.00      0.00         0\n","QT_TEMPERATURE       0.96      0.99      0.97       278\n","     QT_VOLUME       0.91      0.94      0.93       635\n","   TI_DURATION       0.97      0.98      0.98       613\n","\n","      accuracy                           0.97     36864\n","     macro avg       0.72      0.75      0.73     36864\n","  weighted avg       0.97      0.97      0.97     36864\n","\n","Test Loss: 0.11392459023699782\n","Test Accuracy: 0.9716543158689078\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["test()"]},{"cell_type":"markdown","metadata":{"id":"D7YIn6zsbFtk"},"source":["#**(선택) predict 함수이다.**"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":310,"status":"ok","timestamp":1671011681392,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"ajhENtTODgLX"},"outputs":[],"source":["#예측하고 싶은 텍스트를 넣어주세요.\n","predict_text = '다진 마늘을 넣고 잘게 자른 김치를 섞어주세요.'"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1671011513103,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"e7iahMTihUWU"},"outputs":[],"source":["def predict(sentence):\n","    result = []\n","    inputs = tokenizer(sentence,\n","                        return_offsets_mapping=True, \n","                        padding='max_length', \n","                        truncation=True, \n","                        max_length=MAX_LEN,\n","                        return_tensors=\"pt\")\n","\n","    # move to gpu\n","    ids = inputs[\"input_ids\"].to(device)\n","    mask = inputs[\"attention_mask\"].to(device)\n","    # forward pass\n","    outputs = model(ids, attention_mask=mask)\n","    logits = outputs[0]\n","\n","    active_logits = logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n","    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n","    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","    word_prediction = []\n","    token_prediction = []\n","    pred_dict = {}\n","    token_dict = {}\n","    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n","        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n","            result.append([word_idx, token_pred[0], token_pred[1]]) # \n","            #print(token_pred)\n","            if word_idx not in pred_dict:\n","                pred_dict[word_idx] = set()\n","            if word_idx not in token_dict:\n","                token_dict[word_idx] = \"\"\n","            pred_dict[word_idx].add(token_pred[1])\n","            token_dict[word_idx] += token_pred[0].replace(\"#\", \"\")\n","                \n","    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n","        #only predictions on first word pieces are important\n","        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n","            if token_pred[1] != 'O':\n","                token_prediction.append([word_idx, token_pred[0],token_pred[1]])\n","    \n","    #for i in range(len(token_prediction)):\n","    #    print(token_prediction[i], word_idx)\n","    #print()\n","    return result"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671011684323,"user":{"displayName":"소프트웨어학과/송지은","userId":"15070412098287679741"},"user_tz":-540},"id":"0JfupT4vYgV0","outputId":"66413dea-0d62-4e07-8526-6cb0906829b8","scrolled":false},"outputs":[{"data":{"text/plain":["[[0, '다진', 'CV_STATE'],\n"," [1, '마늘', 'CV_INGREDIENT'],\n"," [1, '##을', 'O'],\n"," [2, '넣', 'O'],\n"," [2, '##고', 'O'],\n"," [3, '잘', 'CV_STATE'],\n"," [3, '##게', 'CV_STATE'],\n"," [4, '자', 'CV_STATE'],\n"," [4, '##른', 'CV_STATE'],\n"," [5, '김치', 'CV_INGREDIENT'],\n"," [5, '##를', 'O'],\n"," [6, '섞', 'O'],\n"," [6, '##어', 'O'],\n"," [6, '##주', 'O'],\n"," [6, '##세요', 'O'],\n"," [7, '.', 'O']]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["predict(predict_text)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1mKWd35SE7NoV56Ph1rCSOJanbmnMi5cN","timestamp":1671010203854},{"file_id":"1GsDbxka0nsk8ybzfGo3WkgGkyLfhofzE","timestamp":1658803992993},{"file_id":"1mjk2Nzdm__GnSUcjiJCAGeEWVeS7B9tl","timestamp":1658414474037}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
